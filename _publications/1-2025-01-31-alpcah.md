---
title: "ALPCAH: Subspace Learning for Sample-wise Heteroscedastic Data"
collection: publications
permalink: /publication/1-2025-01-31-alpcah
excerpt: 'Principal component analysis (PCA) is a key tool in the field of data dimensionality reduction. However, some applications involve heterogeneous data that vary in quality due to noise characteristics associated with each data sample. Heteroscedastic methods aim to deal with such mixed data quality. This paper develops a subspace learning method, named ALPCAH, that can estimate the sample-wise noise variances and use this information to improve the estimate of the subspace basis associated with the low-rank structure of the data. Our method makes no distributional assumptions of the low-rank component and does not assume that the noise variances are known. Further, this method uses a soft rank constraint that does not require subspace dimension to be known. Additionally, this paper develops a matrix factorized version of ALPCAH, named LR-ALPCAH, that is much faster and more memory efficient at the cost of requiring subspace dimension to be known or estimated. Simulations and real data experiments show the effectiveness of accounting for data heteroscedasticity compared to existing algorithms. Code available at https://github.com/javiersc1/ALPCAH.'
date: 2025-01-31
venue: 'IEEE Transactions on Signal Processing (TSP)'
paperurl: 'http://javiersc1.github.io/files/paper_alpcah_journal.pdf'
citation: 'J. Salazar Cavazos, J. A. Fessler and L. Balzano, "ALPCAH: Subspace Learning for Sample-wise Heteroscedastic Data," in IEEE Transactions on Signal Processing, doi: 10.1109/TSP.2025.3537867. keywords: {Heteroscedastic data; heterogeneous data quality; subspace basis estimation; subspace learning}'
---
Principal component analysis (PCA) is a key tool in the field of data dimensionality reduction. However, some applications involve heterogeneous data that vary in quality due to noise characteristics associated with each data sample. Heteroscedastic methods aim to deal with such mixed data quality. This paper develops a subspace learning method, named ALPCAH, that can estimate the sample-wise noise variances and use this information to improve the estimate of the subspace basis associated with the low-rank structure of the data. Our method makes no distributional assumptions of the low-rank component and does not assume that the noise variances are known. Further, this method uses a soft rank constraint that does not require subspace dimension to be known. Additionally, this paper develops a matrix factorized version of ALPCAH, named LR-ALPCAH, that is much faster and more memory efficient at the cost of requiring subspace dimension to be known or estimated. Simulations and real data experiments show the effectiveness of accounting for data heteroscedasticity compared to existing algorithms. Code available at https://github.com/javiersc1/ALPCAH.

[Download paper here](http://javiersc1.github.io/files/paper_alpcah_journal.pdf)
